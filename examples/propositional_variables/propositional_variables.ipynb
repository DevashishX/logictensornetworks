{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging; logging.basicConfig(level=logging.INFO)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import ltn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ltn.Proposition(0.2,trainable=True)\n",
    "b = ltn.Proposition(0.5,trainable=True)\n",
    "c = ltn.Proposition(0.5,trainable=True)\n",
    "w1 = ltn.Proposition(0.3, trainable=False)\n",
    "w2 = ltn.Proposition(0.9, trainable=False)\n",
    "\n",
    "x = ltn.Variable(\"x\", np.array([[1,2],[3,4],[5,6]]))\n",
    "P = ltn.Predicate.MLP(input_shapes=[(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=5),semantics=\"forall\")\n",
    "Exists = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMean(p=10),semantics=\"exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_aggregator = ltn.Wrapper_Formula_Aggregator(ltn.fuzzy_ops.Aggreg_Mean())\n",
    "\n",
    "@tf.function\n",
    "def axioms():\n",
    "    axioms = [\n",
    "        # [ (A and B and (forall x: P(x))) -> Not C ] and C\n",
    "        And(\n",
    "            Implies(And(And(a,b),Forall(x,P(x))),\n",
    "                    Not(c)),\n",
    "            c\n",
    "        ),\n",
    "        # w1 -> (forall x: P(x))\n",
    "        Implies(w1, Forall(x,P(x))),\n",
    "        # w2 -> (Exists x: P(x))\n",
    "        Implies(w2, Exists(x,P(x)))\n",
    "    ]\n",
    "    sat_level = formula_aggregator(axioms).tensor\n",
    "    return sat_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Sat Level 0.575\n",
      "Epoch 100: Sat Level 0.678\n",
      "Epoch 200: Sat Level 0.737\n",
      "Epoch 300: Sat Level 0.740\n",
      "Epoch 400: Sat Level 0.742\n",
      "Epoch 500: Sat Level 0.743\n",
      "Epoch 600: Sat Level 0.743\n",
      "Epoch 700: Sat Level 0.743\n",
      "Epoch 800: Sat Level 0.743\n",
      "Epoch 900: Sat Level 0.743\n",
      "Training finished at Epoch 999 with Sat Level 0.743\n"
     ]
    }
   ],
   "source": [
    "trainable_variables = ltn.as_tensors([a,b,c])\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = 1. - axioms()\n",
    "    grads = tape.gradient(loss_value, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "    if epoch%100 == 0:\n",
    "        print(\"Epoch %d: Sat Level %.3f\"%(epoch, axioms()))\n",
    "print(\"Training finished at Epoch %d with Sat Level %.3f\"%(epoch, axioms()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
